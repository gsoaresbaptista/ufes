{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRRuMOYKLB1S"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -q wandb -U\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "wandb_project = \"themis-instruct\"\n",
        "\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
      ],
      "metadata": {
        "id": "GWWBA9dRXKHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "MXJ_XnpQXVo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Data"
      ],
      "metadata": {
        "id": "h2syYXkyNpY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import userdata, drive\n",
        "\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
        "from datetime import datetime\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "UEKnM0PlLhis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BrMNnhmqLGOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open sqlite database as pandas dataframe\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/dataset/questions.db')\n",
        "df = pd.read_sql_query('SELECT * FROM questions', conn)\n",
        "\n",
        "# split into train and eval\n",
        "train_length = int(0.8 * df.shape[0])\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True).drop('id', axis=1)\n",
        "df_train = df.iloc[:train_length, :].copy()\n",
        "df_eval = df.iloc[train_length:, :].copy()\n",
        "del df\n",
        "\n",
        "# show dataframe\n",
        "display(df_train.head())"
      ],
      "metadata": {
        "id": "zAYs2Mv6NhC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "eval_dataset = Dataset.from_pandas(df_eval)"
      ],
      "metadata": {
        "id": "3n2rvmLRMPMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Setup"
      ],
      "metadata": {
        "id": "O7Zy8esLNknN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"dominguesm/canarim-7b-instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "AgC4r41QNnW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  base_model_id,\n",
        "  quantization_config=bnb_config,\n",
        ")"
      ],
      "metadata": {
        "id": "sCiGveViQyKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts_list = [\n",
        "    \"Explique em detalhes:\",\n",
        "    \"Forneça uma resposta abrangente para:\",\n",
        "    \"Analise a seguinte questão:\",\n",
        "    \"Dê uma visão geral de:\",\n",
        "    \"Responda à pergunta abaixo:\",\n",
        "    \"Forneça uma resposta clara e concisa para a seguinte questão:\",\n",
        "    \"Elabore sobre a seguinte pergunta:\",\n",
        "    \"Dê sua perspectiva sobre a seguinte questão:\",\n",
        "    \"Explique detalhadamente a seguinte pergunta:\",\n",
        "    \"Analise a pergunta a seguir:\",\n",
        "    \"Descreva sua interpretação da pergunta abaixo:\",\n",
        "    \"Aborde a seguinte pergunta de maneira abrangente:\",\n",
        "    \"Comente sobre a importância da pergunta a seguir:\",\n",
        "    \"Sumarize suas principais conclusões em relação à pergunta abaixo:\",\n",
        "    \"Explique como a pergunta a seguir impacta o tema:\",\n",
        "    \"Responda à seguinte questão com detalhes:\",\n",
        "    \"Dê uma visão geral da resposta à pergunta abaixo:\",\n",
        "    \"Discuta as razões por trás da pergunta seguinte:\",\n",
        "    \"Descreva a relação entre sua resposta e a pergunta abaixo:\",\n",
        "    \"Compare e contrasta sua abordagem com a pergunta a seguir:\",\n",
        "    \"Examine criticamente a seguinte pergunta:\",\n",
        "    \"Apresente uma perspectiva sólida sobre a pergunta abaixo:\",\n",
        "    \"Ilustre sua resposta com exemplos relacionados à pergunta seguinte:\",\n",
        "    \"Destaque as diferenças fundamentais entre sua resposta e a pergunta abaixo:\",\n",
        "    \"Apresente argumentos convincentes a favor e contra a pergunta seguinte:\",\n",
        "    \"Contextualize a importância da pergunta a seguir:\",\n",
        "    \"Relate como sua resposta se relaciona à pergunta abaixo:\",\n",
        "    \"Considere as implicações de longo prazo da pergunta seguinte:\",\n",
        "    \"Esboce um plano claro para abordar a pergunta abaixo:\",\n",
        "    \"Apresente soluções para a pergunta abaixo:\",\n",
        "    \"Explique as nuances envolvidas na pergunta abaixo:\",\n",
        "    \"Apresente evidências convincentes que sustentem sua resposta à pergunta seguinte:\",\n",
        "    \"Sugira maneiras práticas de melhorar sua abordagem à pergunta abaixo:\",\n",
        "    \"Analise criticamente a teoria subjacente à pergunta seguinte:\",\n",
        "]\n",
        "\n",
        "\n",
        "def format_instruction(sample):\n",
        "    instruction = random.choice(prompts_list)\n",
        "    return f\"\"\"### Instruction:\n",
        "{instruction}\n",
        "\n",
        "### Input:\n",
        "{sample['question']}\n",
        "\n",
        "### Response: {sample['answer']}\"\"\"\n",
        "\n",
        "\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    return tokenizer(format_instruction(prompt))"
      ],
      "metadata": {
        "id": "hbs0Uh37N0G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "1TF3663NTamA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(f'{tokenizer.eos_token}', return_tensors=\"pt\")\n",
        "print(f\"encoded_input {encoded_input}\")"
      ],
      "metadata": {
        "id": "g6lXGhjl1a7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
        "tokenized_eval_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
      ],
      "metadata": {
        "id": "FCrbHKQQN9Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_eval_dataset)"
      ],
      "metadata": {
        "id": "a8zy1eTLOQ2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 1024\n",
        "\n",
        "def generate_and_tokenize_prompt2(prompt):\n",
        "    result = tokenizer(\n",
        "        format_instruction(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "wDAUisXWVHAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt2)\n",
        "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt2)\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ],
      "metadata": {
        "id": "vG7b_-21Up9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Up LoRA"
      ],
      "metadata": {
        "id": "rK35D2H0Vm2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "r5Yy_ovKVpVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "GuElyv60VrHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "H_CE3YSIVv3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerator"
      ],
      "metadata": {
        "id": "D4Bl99LVW4lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ],
      "metadata": {
        "id": "VB5LEGitW5Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "id": "8x7a_h2TW7vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_name = \"themis\"\n",
        "run_name = wandb_project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    callbacks=[transformers.EarlyStoppingCallback(7)],\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        load_best_model_at_end=True,\n",
        "        max_steps=300,\n",
        "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
        "        lr_scheduler_type='cosine',\n",
        "        weight_decay=0.01,\n",
        "        bf16=True,  # CHANGE\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        # eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\" # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_62ltLkmW9VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('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')\""
      ],
      "metadata": {
        "id": "5liIFX7wiLdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(f'gsoaresbaptista/{wandb_project}')"
      ],
      "metadata": {
        "id": "uMnna_Endewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save artifacts\n",
        "#trainer.model.save_pretrained(\"final_checkpoint\")\n",
        "#tokenizer.save_pretrained(\"final_checkpoint\")\n",
        "\n",
        "# Flush memory\n",
        "#del trainer, model\n",
        "#gc.collect()\n",
        "new_model = 'themis-instruct-qa'\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Reload model in FP16 (instead of NF4)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
        "\n",
        "# Merge base model with the adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"final_checkpoint\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save model and tokenizer\n",
        "model.save_pretrained(new_model)\n",
        "tokenizer.save_pretrained(new_model)\n",
        "\n",
        "# Push them to the HF Hub\n",
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "o49DrPuSdsEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "VCntaaqbemWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E3z5WHX_97la"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}